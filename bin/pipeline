#!/usr/bin/env python

import os
import sys
import argparse
import subprocess

sys.path.append(os.path.abspath(os.getcwd() + '/../'))
sys.path.append('/usr/local/lib/python2.7/site-packages')

from sklearn.metrics import precision_recall_curve, auc, classification_report, precision_recall_fscore_support
from sklearn.externals import joblib
from BeautifulSoup import BeautifulSoup

from beta.src import utils
from beta.src.preprocessing import Processor
from beta.src.evaluate import Evaluate

def main(args):
    # get extractor path
    extractor = os.path.join(utils.get_path('bin'), 'extractor')
    # generate site id
    site_id   = utils.process_url(args.url.replace('\\', ''))['site_id']

    # generate command
    command = extractor + ' -u %s -i true'

    #log
    utils.log('[pipeline] Executing Extractor ...', 'system')
    # execute extractor
    subprocess.call(command % args.url, shell=True)

    # get model path
    model = os.path.join(utils.get_path('models'), args.model + '.pkl')
    # get vectorizer path
    vect  = os.path.join(utils.get_path('models'), args.model + '-vectorizer.pkl')

    # log
    utils.log('[pipeline] Loading trained model and vectorizer %s ...' % args.model, 'system')

    # trained model
    classifier = None
    # saved vectorizer
    vectorizer = None

    try:
        # load model
        classifier = joblib.load(model)
        # load vectorizer
        vectorizer = joblib.load(vect)

        # log
        utils.log('[pipeline] Model %s has been loaded.' % args.model, 'success')
    except:
        utils.log('[pipeline] An error occured while loading the model, Model does not exists.', 'error')
        sys.exit()

    # log
    utils.log('[pipeline] Loading extracted data %s ...' % site_id, 'system')

    # get tmp folder
    tmp = os.path.join(utils.get_path('tmp'), site_id + '.json')

    try:
        # load extracted data
        testing_data = utils.read_file(tmp, True)

        # log
        utils.log('[pipeline] Extracted data %s has been loaded.' % site_id, 'success')
    except:
        # log
        utils.log('[pipeline] Unable to load extracted data.', 'error')
        sys.exit()

    # start processing data
    process(classifier, testing_data, vectorizer, args.output)

# process extracted data
def process(classifier, testing_data, vectorizer, output):
    # log
    utils.log('[pipeline] Processing extracted data ...', 'system')

    # prepare original features
    texts = Processor().prepare(testing_data['texts'])

    # prepare testing data
    testing_processed = Processor().prepare_testing(testing_data['texts'], vectorizer)

    # get the features
    testing_features = testing_processed['features']

    # log
    utils.log('[pipeline] Extracted data has been processed.', 'success')

    # log
    utils.log('[pipeline] Starting predictions ...', 'system')

    # start prediction
    results = classifier.predict(testing_features)

    # log
    utils.log('[pipeline] Prediction completed, processing predictions ...', 'success')

    # predicted data
    predicted = {}
    # index
    index     = 0

    # iterate on each results
    for i in results:
        # not unknown?
        if i != 'unknown':
            # get the text
            text = testing_data['texts'][index]['text']

            # if not yet set
            if not i in predicted:
                predicted[i] = []

            # check ogprop
            if not 'ogprop' in testing_data['texts'][index]:
                testing_data['texts'][index]['ogprop'] = ''

            soup = BeautifulSoup(testing_data['texts'][index]['html'])

            # generate information
            data = {
                'label'     : i,
                'tag'       : testing_data['texts'][index]['element']['name'],
                'classes'   : testing_data['texts'][index]['element']['classes'],
                'og'        : testing_data['texts'][index]['ogprop'],
                'computed'  : texts['features'][index]
            }

            # is it title?
            if i == 'title':
                data['text'] = text[0]
            # or description?
            elif i == 'description':
                data['html'] = testing_data['texts'][index]['html']
                data['text'] = soup.getText()
            else:
                data['text'] = text[0]

            # append the information
            predicted[i].append(data)

        index = index + 1

    # log
    utils.log('[pipeline] Evaluating results ...', 'system')

    # start scoring results
    scored = Evaluate(testing_data, predicted, results).score(min_score=5.0)

    # log
    utils.log('[pipeline] Results has been scored and evaluated ...', 'success')

    # log
    utils.pretty(scored)

    # are we going to save the output?
    if output != None:
        # file exists?
        if not utils.file_exists(output):
            # log
            utils.log('[pipeline] Unable to save output to %s, path does not exists.', 'error')

            # exit
            sys.exit()

        # write that data
        utils.write_file(output, scored, True)

        # log
        utils.log('[pipeline] Output saved to %s' % output, 'success')

# argument parser
def parse_args():
    # initialize argument parser
    parser = argparse.ArgumentParser(prog='pipeline', description='Extraction Pipeline')

    # set url argument
    parser.add_argument('-u', '--url', nargs='?', const=str, required=True, help='webpage url to extract e.g http://somesite.ph/product/124')
    # set model argument
    parser.add_argument('-m', '--model', nargs='?', const=int, required=False, help='trained model to be used for prediction')
    # set output of results
    parser.add_argument('-o', '--output', nargs='?', const=str, required=False, help='dump predicted results to this path')

    return parser.parse_args()

if __name__ == '__main__':
    main(parse_args());